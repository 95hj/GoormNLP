{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[HW24_Problem] NaiveBayes Classifier.ipynb","provenance":[{"file_id":"1FGrU6Euc_-sM8mf7GTSYYdBdtQhu2SGv","timestamp":1645751751715}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"btDmoiRCRfMp"},"source":["# **[HW24] NaiveBayes Classifier**\n","1. Requirements\n","2. Data Preprocessing\n","3. Model Training\n","4. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"3a3E1pbkSYSF"},"source":["## 1. Requirements"]},{"cell_type":"markdown","metadata":{"id":"4THGPr9QIjQY"},"source":["#### 1.1 필요한 패키지를 설치(install) 및 import 합니다."]},{"cell_type":"code","metadata":{"id":"6GKm6PwhiGxv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752130150,"user_tz":-540,"elapsed":8098,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"3a7eda0c-755e-4676-e175-dd25e1a2535c"},"source":["# 한국어 전처리 라이브러리 \n","!pip install konlpy"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 47.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","metadata":{"id":"2srhF-lp4JxL","executionInfo":{"status":"ok","timestamp":1645752130150,"user_tz":-540,"elapsed":7,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}}},"source":["from tqdm import tqdm\n","from collections import defaultdict\n","import math\n","\n","# POS(Part of Speech) tagger\n","from konlpy import tag "],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ-s9c77IVUA"},"source":["#### 1.2 Train data 와 test data 를 준비합니다."]},{"cell_type":"code","metadata":{"id":"MCBnEQrfoL_C","executionInfo":{"status":"ok","timestamp":1645752130151,"user_tz":-540,"elapsed":7,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}}},"source":["data = {}\n","# training data. input text 와 정답 label (긍정(1), 부정(0)) 로 구성.\n","\n","data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n","                {'text': \"기대했던 것보단 별로였네요.\"},\n","                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n","                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n","                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n","                {'text': \"연기가 좀 별로였습니다.\"},\n","                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n","                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n","                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n","                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n","                ]\n","# test data\n","data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n","                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n","                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n","                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n","                ]\n","\n","train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n","test_labels = [1, 0, 1, 0]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpgjbzqr6UR4"},"source":["### 2. Data Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"wKwnFBZqMXm-"},"source":["#### 2.1 한글 형태소 분석기를 이용해서 주어진 데이터를 tokenize 합니다.\n","\n","오픈소스 형태소 분석기를 제공하는 파이썬 패키지 KoNLPy에서 제공하는 [꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용하여 tokenize 합니다."]},{"cell_type":"code","metadata":{"id":"bEzeYDmPjNLl","executionInfo":{"status":"ok","timestamp":1645752132227,"user_tz":-540,"elapsed":2082,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}}},"source":["# 형태소 분석기 선언\n","morph_analyzer = tag.Kkma() "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tftxirx_7sk7","executionInfo":{"status":"ok","timestamp":1645752132227,"user_tz":-540,"elapsed":7,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}}},"source":["# tokenization 함수 정의\n","def tokenization(data, morph_analyzer):\n","    '''\n","    (input) data: list of data examples.\n","            morph_analyzer: morphological analyzer.\n","    (output) tokenized_data: list of tokenized data examples.\n","    '''\n","    tokenized_data = []\n","\n","    for example in tqdm(data):\n","        tokens = morph_analyzer.morphs(example['text'])\n","        tokenized_data.append(tokens)\n","\n","    return tokenized_data"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4VEZyFlCqi-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752148054,"user_tz":-540,"elapsed":15833,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"5e6b9c21-24e0-467e-d657-9d55a66a44f1"},"source":["# tokenization 함수를 이용한 데이터 tokenization\n","tokenized_data = {}\n","\n","tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n","tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n","100%|██████████| 4/4 [00:00<00:00,  9.75it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"OEhn3uv2o2kt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752148055,"user_tz":-540,"elapsed":5,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"58c6e9cb-4c1e-4730-dbae-e851f2bf290c"},"source":["# tokenized_data 확인\n","tokenized_data['train']"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n"," ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n"," ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n"," ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n"," ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n"," ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n"," ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n"," ['기념일',\n","  '에',\n","  '방문',\n","  '하',\n","  '었',\n","  '는데',\n","  '연기',\n","  '도',\n","  '연출',\n","  '도',\n","  '다',\n","  '좋',\n","  '았',\n","  '습니다',\n","  '.'],\n"," ['전반적',\n","  '으로',\n","  '지루',\n","  '하',\n","  '었',\n","  '습니다',\n","  '.',\n","  '저',\n","  '는',\n","  '별',\n","  '로',\n","  '이',\n","  '었',\n","  '네요',\n","  '.'],\n"," ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"olCz4j6VS0KJ"},"source":["#### 2.2 tokenization 결과를 이용해서 word to index dictionary 를 생성합니다.\n"]},{"cell_type":"code","metadata":{"id":"u2DXYDTTTChQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752149207,"user_tz":-540,"elapsed":1154,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"d8cecf4d-e9ae-42c5-eb07-fd106190fce0"},"source":["# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n","tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i] ]\n","unique_train_tokens = set(tokens)\n","\n","# NaiveBayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n","word2index = defaultdict() # key: word, value: index of word\n","idx = 0\n","for token in tqdm(unique_train_tokens):\n","    word2index[token] = idx\n","    idx += 1"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:00<00:00, 74048.24it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"85oCOe0Xqcwk"},"source":["### 3. Model Training"]},{"cell_type":"markdown","metadata":{"id":"w3uuFi52qjh6"},"source":["#### 3.1 NaiveBayes Classifier 모델 클래스를 구현합니다.\n"]},{"cell_type":"code","metadata":{"id":"TsZlgjkBHAod","executionInfo":{"status":"ok","timestamp":1645752242340,"user_tz":-540,"elapsed":494,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}}},"source":["class NaiveBayesClassifier():\n","    def __init__(self, word2index, k=0.1):\n","        \"\"\"\n","        (input) word2index: mapping a word to a pre-assigned index.\n","        \"\"\"\n","        self.k = k # for smoothing\n","        self.word2index = word2index\n","        self.priors = {} # Prior probability for each class, P(c)\n","        self.likelihoods = {} # Likelihood for each token, P(d|c)\n","\n","    def _set_priors(self, labels):\n","        \"\"\"\n","        Set prior probability for each class, P(c).\n","        Count the number of each class and calculate P(c) for each class.\n","        \"\"\"\n","        \n","        class_counts = defaultdict(int)\n","        ############################ ANSWER HERE ################################\n","        # TODO 1: Count the number of each class\n","        class_counts = defaultdict(int)\n","        for label in tqdm(labels):\n","            class_counts[label] += 1\n","        # TODO 2: For each class, calcuate P(c)\n","        for label, count in class_counts.items():\n","            self.priors[label] = class_counts[label] / len(labels)\n","        #########################################################################        \n","\n","    def _set_likelihoods(self, tokens, labels):\n","        \"\"\"\n","        Set likelihood for each token, P(d|c).\n","        First, count the number of each class for each token.\n","        Then, calculate P(d|c) for a given class and token.\n","        \"\"\"\n","        token_dists = {}\n","        class_counts = defaultdict(int)\n","\n","        for i, label in enumerate(tqdm(labels)):\n","            count = 0\n","\n","            ############################ ANSWER HERE ################################\n","            # TODO: Count the number of each class for each token.\n","            for token in tokens[i]:\n","                if token in self.word2index:\n","                    if token not in token_dists:\n","                        token_dists[token] = {0:0, 1:0}\n","                    token_dists[token][label] += 1\n","                count += 1\n","            class_counts[label] += count\n","            #########################################################################        \n","\n","        for token, dist in tqdm(token_dists.items()):\n","            if token not in self.likelihoods:\n","                self.likelihoods[token] = {\n","                    0: (token_dists[token][0]+ self.k) / (class_counts[0] + len(self.word2index)* self.k),\n","                    1: (token_dists[token][1]+ self.k) / (class_counts[1] + len(self.word2index)* self.k),\n","                }\n","\n","    def train(self, input_tokens, labels):\n","        \"\"\"\n","        (input) input_tokens: list of tokenized train data.\n","                labels: train labels for each sentence/document.\n","        \"\"\"\n","        self._set_priors(labels)\n","        self._set_likelihoods(input_tokens, labels)\n","\n","    def inference(self, input_tokens):\n","        \"\"\"\n","        (input) input_tokens: list_of tokenized test data.\n","        \"\"\"\n","        log_prob_0 = 0.0\n","        log_prob_1 = 0.0\n","\n","        for token in input_tokens:\n","            if token in self.likelihoods:\n","                log_prob_0 += math.log(self.likelihoods[token][0])\n","                log_prob_1 += math.log(self.likelihoods[token][1])\n","\n","        log_prob_0 += math.log(self.priors[0])\n","        log_prob_1 += math.log(self.priors[1])\n","\n","        if log_prob_0 >= log_prob_1:\n","            return 0\n","        else:\n","            return 1"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzjVUyBOQEJk"},"source":["#### 3.2 주어진 학습 데이터에 대해 문장 분류 모델을 학습시킵니다."]},{"cell_type":"code","metadata":{"id":"Wt-iUEVRNsRj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752246389,"user_tz":-540,"elapsed":476,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"a280e2b2-2336-4baf-9f51-be970f0aff02"},"source":["# 문장 분류 모델 선언 및 학습\n","classifier = NaiveBayesClassifier(word2index)\n","classifier.train(tokenized_data['train'], train_labels)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 57614.07it/s]\n","100%|██████████| 10/10 [00:00<00:00, 40563.87it/s]\n","100%|██████████| 56/56 [00:00<00:00, 184509.84it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"h79XWrsnQJtN"},"source":["### 4. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Pjk05W136d5o"},"source":["각각의 Test 데이터에 대해 정답값을 추론하고 Accuracy를 구합니다."]},{"cell_type":"code","metadata":{"id":"Fe-fOScGNzH3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752256900,"user_tz":-540,"elapsed":477,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"95510f7b-fefb-46e0-c3ef-3460d7e8fcde"},"source":["# Test 데이터 inference\n","preds = []\n","for test_tokens in tqdm(tokenized_data['test']):\n","    pred = classifier.inference(test_tokens)\n","    preds.append(pred)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 15635.80it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"hrYMTKM10vYk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645752259037,"user_tz":-540,"elapsed":648,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"0a6b0807-b38e-470b-919c-2d35af10b347"},"source":["# Accuracy 측정\n","from sklearn.metrics import accuracy_score\n","\n","print(accuracy_score(test_labels, preds))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n"]}]},{"cell_type":"code","metadata":{"id":"4b68-Bh5zArE","executionInfo":{"status":"aborted","timestamp":1645752151173,"user_tz":-540,"elapsed":5,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}}},"source":[""],"execution_count":null,"outputs":[]}]}