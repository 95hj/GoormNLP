{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Quiz3.ipynb","provenance":[{"file_id":"1GCsOB7wB4ZEikMlsVSsbloUu0qRX0stD","timestamp":1648170045935}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"pGdFkr3AJtsB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648175340722,"user_tz":-540,"elapsed":5880,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"bb66033e-178a-49b7-b0ae-f60f8f7b6af7"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"UVEUJ5AVJp9K"},"source":["다음 주관식 질문에 올바른 답을 작성하시오."]},{"cell_type":"markdown","metadata":{"id":"5Dkp5OZfJulS"},"source":["1. bert-base-uncased의 tokenizer를 이용해 \"I love natural language processing\"을 tokenize한 뒤 token, id, string을 반환하는 코드를 transformers 라이브러리를 이용해 작성하세요."]},{"cell_type":"code","metadata":{"id":"UsXfvF0DLX7F"},"source":["from transformers import AutoTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"EogaPj2amt74"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5WTkwS4NTT8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648175346164,"user_tz":-540,"elapsed":5,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"7f93b348-f104-40ab-f1d0-475b9a513e5f"},"source":["print(tokenizer.tokenize(\"I love natural language processing\"))\n","print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I love natural language processing\")))\n","print(tokenizer.convert_tokens_to_string(tokenizer.tokenize(\"I love natural language processing\")))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'love', 'natural', 'language', 'processing']\n","[1045, 2293, 3019, 2653, 6364]\n","i love natural language processing\n"]}]},{"cell_type":"markdown","metadata":{"id":"6ooI72AZPArI"},"source":["2. transformers의 pipeline class를 이용해 bert-base-uncased 모델을 load하고 \"I [MASK] natural language processing\"에서 \"[MASK]\"에 대한 예측값 상위 5개를 출력하는 코드를 작성하세요"]},{"cell_type":"code","metadata":{"id":"bTxtdeEvMfmT"},"source":["from transformers import pipeline, AutoTokenizer, AutoModel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7iUSURdLmco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648175361662,"user_tz":-540,"elapsed":5126,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"29aed7c4-a24b-4041-a463-776599ab6e1a"},"source":["classifier_ = pipeline(\"fill-mask\")\n","classifier_(\"I <mask> natural language processing\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.1383543610572815,\n","  'sequence': 'Ironic natural language processing',\n","  'token': 18731,\n","  'token_str': 'ronic'},\n"," {'score': 0.12347400188446045,\n","  'sequence': 'I prefer natural language processing',\n","  'token': 6573,\n","  'token_str': ' prefer'},\n"," {'score': 0.09194808453321457,\n","  'sequence': 'IEEE natural language processing',\n","  'token': 47146,\n","  'token_str': 'EEE'},\n"," {'score': 0.03860380873084068,\n","  'sequence': 'I love natural language processing',\n","  'token': 657,\n","  'token_str': ' love'},\n"," {'score': 0.03268039971590042,\n","  'sequence': 'I am natural language processing',\n","  'token': 524,\n","  'token_str': ' am'}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"FJH8r7exVqIY"},"source":["3. bert-base-uncased의 tokenizer에서 1002번째 token의 string을 출력하는 코드를 작성하세요."]},{"cell_type":"code","metadata":{"id":"9-1yCvmOVpqM"},"source":["from transformers import AutoTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZ16hCfve--v"},"source":["tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLy3bYMFPzlI"},"source":["token = tokenizer.convert_ids_to_tokens(1002)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chzlbUsUWvfw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648175362899,"user_tz":-540,"elapsed":5,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"3a6b5693-b391-4381-ca21-b6f88eba209f"},"source":["print(tokenizer.convert_tokens_to_string(token))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["$\n"]}]},{"cell_type":"markdown","metadata":{"id":"UAYBe3kkmXrz"},"source":["4. 다음 출력 문장과 정답 문장을 이용해 BLEU score를 계산하시오. (BLEU score 계산법은 강의자료를 따라주세요.) \n","\n","Brevity Score: 0.5  \n","Predicted Sentence: I like nlp but not you  \n","Reference: I love natural language processing how about you ?"]},{"cell_type":"markdown","source":["length of reference = 9  \n","lenght of prediction = 6  \n","precision (1-gram) = 2/6  \n","precision (2-gram) = 0  \n","precision (3-gram) = 0  \n","precision (4-gram) = 0  \n","brevity penalty = 6/9  \n","BLEU score = 0  \n","\n","---"],"metadata":{"id":"l-Z3JqB8p8ED"}},{"cell_type":"markdown","metadata":{"id":"TyHBp2luIurU"},"source":["다음 명제에 대해, True/False를 판단하시오. 판단 근거를 간략하게 설명하시오."]},{"cell_type":"markdown","metadata":{"id":"BqX6eEzXIzqJ"},"source":["1. GPT-1,2,3 은 Bi-directional Transformer이다."]},{"cell_type":"markdown","source":["False. Transformer의 Decoder를 사용하여 sequence에서 뒤의 token들은 making한다.  \n","\n","---"],"metadata":{"id":"BOZwHVKIrqk4"}},{"cell_type":"markdown","metadata":{"id":"D7EyLImgj18s"},"source":["2. GPT-1은 여러가지의 task를 하나의 모델로 수행할 수 있다."]},{"cell_type":"markdown","source":["True. pretrain 후에 여러 task에 대해 fine-tuning 할 수 있다.  \n","\n","---"],"metadata":{"id":"Dz1asTtYryxE"}},{"cell_type":"markdown","metadata":{"id":"D7Y_2sSlkRUU"},"source":["3. BERT는 classification을 하기 위해 encoder(transformer block)의 output을 모두 활용한다."]},{"cell_type":"markdown","source":["False. [CLS] token을 사용한다.  \n","\n","---"],"metadata":{"id":"ZqA1tzRzswqg"}},{"cell_type":"markdown","metadata":{"id":"10bqA1ivlBme"},"source":["4. BERT의 [CLS] token은 반드시 첫번째 sequence에 위치 해야한다."]},{"cell_type":"markdown","source":["True. 입력과 출력 모두 맨 앞에 [CLS]token이 위치한다.  \n","\n","---"],"metadata":{"id":"aHXBB5dOs2ww"}},{"cell_type":"markdown","metadata":{"id":"bj0wFqQjl8TS"},"source":["5. 두 문장의 의미가 서로 같은지 다른지 판단하는 task를 BERT를 이용하여 수행할 때 문장의 위치가 서로 바뀌더라도 성능은 동일하다."]},{"cell_type":"markdown","source":["False. BERT는 Bi-directional Transformer 모델이지만 입력 token들에 position embedding의 정보가 함께 들어가기 때문이다.\n","\n","---"],"metadata":{"id":"mrOS4m2etFhL"}},{"cell_type":"markdown","metadata":{"id":"8BdOO525d_Tu"},"source":["### build_bpe 함수를 완성해주세요.\n","\n","byte pair encoding을 이용한 간단한 sub-word tokenizer를 구현해봅니다.\n","과제 노트북의 지시사항과 각 함수의 docstring과 [논문](https://arxiv.org/pdf/1508.07909.pdf)의 3페이지 algorithm 1 참고하여 build_bpe 함수를 완성하고 모든 test case를 통과해주세요."]},{"cell_type":"code","metadata":{"id":"sGOaLtUQd4D1"},"source":["from typing import List, Dict, Set\n","from itertools import chain\n","import re\n","from collections import defaultdict, Counter\n","\n","\n","def build_bpe(\n","        corpus: List[str],\n","        max_vocab_size: int\n",") -> List[int]:\n","    \"\"\" BPE Vocabulary Builder\n","    Implement vocabulary builder for byte pair encoding.\n","    Please sort your idx2word by subword length in descending manner.\n","\n","    Hint: Counter in collection library would be helpful\n","\n","    Note: If you convert sentences list to word frequence dictionary,\n","          building speed is enhanced significantly because duplicated words are\n","          preprocessed together\n","\n","    Arguments:\n","    corpus -- List of words to build vocab\n","    max_vocab_size -- The maximum size of vocab\n","\n","    Return:\n","    idx2word -- Subword list\n","    \"\"\"\n","    # Special tokens\n","    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0\n","    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1\n","    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2\n","    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3\n","    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4\n","    SPECIAL = [PAD, UNK, CLS, SEP, MSK]\n","\n","    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word\n","\n","    idx2word: List[str] = SPECIAL\n","    _corpus = [word + WORD_END for word in corpus]\n","    vocab = []\n","    for word in _corpus:\n","        vocab+=list(word)\n","    vocab = set(vocab)\n","    corpus =[]\n","    for word in _corpus:\n","        letter = list(word)\n","        word_ = ' '.join(letter)\n","        corpus.append(word_)\n","    corpus = Counter(corpus)\n","\n","    '''\n","    Arguments:\n","        vocab: Counter dict\n","    Return:\n","        pairs: dictionary with key as tuple of symbols and value as its count in corpus\n","    '''\n","\n","    def get_stats(vocab):\n","        pairs = defaultdict(int)\n","        for word, freq in vocab.items():\n","            symbols = word.split()\n","            for i in range(len(symbols) - 1):\n","                pairs[(symbols[i], symbols[i + 1])] += freq\n","\n","        return pairs\n","\n","    def merge_vocab(pair, v_in):\n","        v_out = {}\n","        bigram = re.escape(' '.join(pair))\n","        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","        for word in v_in:\n","            w_out = p.sub(''.join(pair), word)\n","            v_out[w_out] = v_in[word]\n","        return v_out\n","\n","    # pair_count_dict: dictionary with key as tuple of symbols and value as its count in corpus\n","    while(len(vocab)+len(SPECIAL) < max_vocab_size):\n","        pair_count_dict = get_stats(corpus)\n","        if len(pair_count_dict) == 0:\n","            break\n","        best = max(pair_count_dict, key=pair_count_dict.get)\n","        vocab.add(best[0]+best[1])\n","        corpus = merge_vocab(best, corpus)\n","        \n","    new_idx2word = list(vocab)\n","    new_idx2word.sort(reverse=True, key=len)\n","    idx2word += new_idx2word\n","    return idx2word\n","\n","def encode(\n","    sentence: List[str],\n","    idx2word: List[str]\n",") -> List[int]:\n","    \"\"\" BPE encoder\n","    Implement byte pair encoder which takes a sentence and gives the encoded tokens\n","    Arguments:\n","    sentence -- The list of words which need to be encoded.\n","    idx2word -- The vocab that you have made on the above build_bpe function.\n","    \n","    Return:\n","    tokens -- The list of the encoded tokens\n","    \"\"\"\n","    WORD_END = BytePairEncoding.WORD_END\n","\n","    def tokenize_word(word, vocab, bias):\n","        UNK = BytePairEncoding.UNK_token\n","        # Define base case to stop recursion\n","        if word == '':\n","            return []\n","        if len(vocab) == 0:\n","            return [UNK]\n","\n","        # i: index for current vocab\n","        # bias + i: index for global vocab\n","        # skip tokens until we find the token that satisfies the pattern\n","        # word = [substring + token]*n + remainder\n","        #  => substrings and remainder are fed  recursively to this function with partial vocab\n","        for i in range(len(vocab)):\n","            token = vocab[i]\n","            pattern = re.escape(token)\n","            matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(pattern, word)]\n","\n","            if len(matched_positions) == 0:\n","                continue\n","\n","            substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n","            substring_start_position = 0\n","\n","            # substring start ~ substring end: recursively tokenize with vocabulary of idx2word[i+1:]\n","            # substring end ~ token[i] length: tokenize into token[i]\n","            # update substring start to be substring end + len(token[i])\n","            # iterate until there is no more matched token in the string\n","            tokens = []\n","            for substring_end_position in substring_end_positions:\n","                substring = word[substring_start_position:substring_end_position]\n","                tokens += tokenize_word(substring, vocab[i + 1:], i + bias + 1)\n","                tokens.append(i + bias)\n","                substring_start_position = substring_end_position + len(token)\n","\n","            # Take care of the remaining substring by recursive call with vocab idex2word[i+1:] then break\n","            remaining_substring = word[substring_start_position:]\n","            tokens += tokenize_word(remaining_substring, vocab[i + 1:], i + bias + 1)\n","            break\n","        return tokens\n","    tokens = []\n","    WORD_END = BytePairEncoding.WORD_END\n","    for word in sentence:\n","        word_tokenized = tokenize_word(word + WORD_END, idx2word, 0)\n","        tokens += word_tokenized\n","\n","    return tokens\n","\n","def decode(\n","    tokens: List[int],\n","    idx2word: List[str]\n",") -> List[str]:\n","    \"\"\" BPE decoder\n","    Implement byte pair decoder which takes tokens and gives the decoded sentence.\n","    Arguments:\n","    tokens -- The list of tokens which need to be decoded\n","    idx2word -- the vocab that you have made on the above build_bpe function.\n","    Return:\n","    sentence  -- The list of the decoded words\n","    \"\"\"\n","    WORD_END = BytePairEncoding.WORD_END\n","    sentence: List[str] = None\n","    sentence = [idx2word[token] for token in tokens]\n","    sentence2 = \"\"\n","    for i in range(len(sentence)):\n","        sentence2 += sentence[i]\n","    sentence = sentence2.split(WORD_END)\n","    if sentence[-1] == \"\":\n","        sentence.pop(-1)\n","    ### END YOUR CODE\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnu8nACHkLuL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648175363442,"user_tz":-540,"elapsed":546,"user":{"displayName":"조현수","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06575509224308139266"}},"outputId":"0d596503-f40e-49ec-e777-cf7f24b0a180"},"source":["#############################################\n","# Helper functions below. DO NOT MODIFY!    #\n","#############################################\n","\n","class BytePairEncoding(object):\n","    \"\"\" Byte Pair Encoding class\n","    We aren't gonna use this class for encoding. Because it is too slow......\n","    We will use sentence piece Google have made.\n","    Thus, this class is just for special token index reference.\n","    \"\"\"\n","    PAD_token = '<pad>'\n","    PAD_token_idx = 0\n","    UNK_token = '<unk>'\n","    UNK_token_idx = 1\n","    CLS_token = '<cls>'\n","    CLS_token_idx = 2\n","    SEP_token = '<sep>'\n","    SEP_token_idx = 3\n","    MSK_token = '<msk>'\n","    MSK_token_idx = 4\n","\n","    WORD_END = '_'\n","\n","    def __init__(self, corpus: List[List[str]], max_vocab_size: int) -> None:\n","        self.idx2word = build_bpe(corpus, max_vocab_size)\n","\n","    def encode(self, sentence: List[str]) -> List[int]:\n","        return encode(sentence, self.idx2word)\n","\n","    def decoder(self, tokens: List[int]) -> List[str]:\n","        return decode(tokens, self.idx2word)\n","\n","\n","#############################################\n","# Testing functions below.                  #\n","#############################################\n","\n","\n","def test_build_bpe():\n","    print(\"======Building BPE Vocab Test Case======\")\n","    PAD = BytePairEncoding.PAD_token\n","    UNK = BytePairEncoding.UNK_token\n","    CLS = BytePairEncoding.CLS_token\n","    SEP = BytePairEncoding.SEP_token\n","    MSK = BytePairEncoding.MSK_token\n","    WORD_END = BytePairEncoding.WORD_END\n","\n","    # First test\n","    corpus = ['abcde']\n","    vocab = build_bpe(corpus, max_vocab_size=15)\n","    assert vocab[:5] == [PAD, UNK, CLS, SEP, MSK], \\\n","        \"Please insert the special tokens properly\"\n","    print(\"The first test passed!\")\n","\n","    # Second test\n","    assert sorted(vocab[5:], key=len, reverse=True) == vocab[5:], \\\n","        \"Please sort your idx2word by subword length in decsending manner.\"\n","    print(\"The second test passed!\")\n","\n","    # Third test\n","    corpus = ['low'] * 5 + ['lower'] * 2 + ['newest'] * 6 + ['widest'] * 3\n","    vocab = set(build_bpe(corpus, max_vocab_size=24))\n","    assert vocab > {PAD, UNK, CLS, SEP, MSK, 'est_', 'low', 'newest_', \\\n","                    'i', 'e', 'n', 't', 'd', 's', 'o', 'l', 'r', 'w',\n","                    WORD_END} and \\\n","           \"low_\" not in vocab and \"wi\" not in vocab and \"id\" not in vocab, \\\n","        \"Your bpe result does not match expected result\"\n","    print(\"The third test passed!\")\n","\n","    # forth test\n","    corpus = ['aaaaaaaaaaaa', 'abababab']\n","    vocab = set(build_bpe(corpus, max_vocab_size=13))\n","    assert vocab == {PAD, UNK, CLS, SEP, MSK, 'aaaaaaaa', 'aaaa', 'abab', 'aa',\n","                     'ab', 'a', 'b', WORD_END}, \\\n","        \"Your bpe result does not match expected result\"\n","    print(\"The forth test passed!\")\n","\n","    # fifth test\n","    corpus = ['abc', 'bcd']\n","    vocab = build_bpe(corpus, max_vocab_size=10000)\n","    assert len(vocab) == 15, \\\n","        \"Your bpe result does not match expected result\"\n","    print(\"The fifth test passed!\")\n","\n","    print(\"All 5 tests passed!\")\n","test_build_bpe()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======Building BPE Vocab Test Case======\n","The first test passed!\n","The second test passed!\n","The third test passed!\n","The forth test passed!\n","The fifth test passed!\n","All 5 tests passed!\n"]}]}]}